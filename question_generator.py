# -*- coding: utf-8 -*-
"""question_generator.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14i4i1KU_nkzCwIakbswnqsYqool3adN8

수능 문제 제작 

1. 반드시 제작된 지문을 바탕으로 할 것.

2. 유형별로 다른 기준을 적용할 것

### 1) 주제 유형

핵심 키워드: Use, members, information, principle, dose,
society, elements 등 

신유형 존재: 문장에 밑줄(주로 속담)을 긋고, 이 글 내에서 무엇을 의미하는지 찾는 문제가 있음.

핵심 구조: 부정문 구조를 자주 사용

### 2) 문법 & 어휘

핵심 키워드: Figure, 사람이름, 자연물이나 스포츠

핵심 구조: 유의어 및 반의어를 주로 사용 

### 3) 분위기 심경

핵심 키워드: 사람 이름, 형용사, 감정 표현

핵심 구조: have to, will, should, would 등의 문장

 
### 4) 빈칸 추론

핵심 키워드: members, society, disagreement, theory, individual, evidence, 사회학, 과학, 인문학 이론, 실험

핵심 구조: 앞 뒤 문장과의 맥락, 해석 자체가 굉장히 어려운 경우가 많아서, 핵심 단어가 답을 결정하는 경우가 생김

### 5) 세부 내용

핵심 키워드: graph, place, percentage, aged, time, date

이미지 & 도표: 시각화 자료 활용방안이 2가지 존재

핵심 구조: 최대 최소 등

 (1) 지문의 자료 기반 --> 시각화 데이터 만들기
 
 (2) 시각화 데이터 기반 --> 지문 만들기
 
 ### 6) 순서
 
 핵심 키워드: math, 사람 이름, 이론
 
 핵심 구조: 상식이 잘 통하지 앟는 어려운 지문
 
 this, these their, them,, others, one less 등의
 
 지시대명사의 중요도가 굉장히 높음
"""

## Pseudo code. 주제 지문

# 0. 필요 패키지 이모트

from __future__ import absolute_import, division, print_function, unicode_literals
from google.colab import drive
from nltk import word_tokenize
from nltk.corpus import stopwords

import time
import numpy as np
import os
import pandas as pd
import re
import nltk


nltk.download('words')
nltk.download('stopwords')
nltk.download('punkt')

# 1. 제작된 주제 지문 데이터 로드

drive.mount('/content/gdrive')

def load_gen_text(filename):
  with open('/content/gdrive/My Drive/Colab Notebooks/{}'.format(filename), 'rb') as f:
    data = f.read().decode(encoding = 'utf-8')
    return data


# 2. 로드한 데이터를 문장 단위로 분절하기. (필요하다면 단어별로 토큰화)
def split_sentence(textname):
  sentences = re.split(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?)\s', textname)
  return sentences


# 2.5 (시도해보기) stopwords 제거 후, 동사 명사 형용사만 남겨보기?

def tokenize_text(textname):
  tokens = word_tokenize(textname)
  return tokens


def content_word(textname):
  stopwords = nltk.corpus.stopwords.words('english')
  content = [w for w in textname if w.lower() not in stopwords]
  return content

## 문장 단위로 content_word 적용

def sentence_content(sentence_split_text):
  text_container = []
  for text_ in sentence_split_text:
    tokens = tokenize_text(text_)
    content = content_word(tokens)
    text_container.append(content)
    
  return text_container


def sentence_all(sentence_split_text):
  text_container = []
  for text_ in sentence_split_text:
    tokens = tokenize_text(text_)
    text_container.append(tokens)
    
  return text_container


# 3. should could can / 부정어구, 부정문 / 빈도수가 높은 핵심 동사 (rank 목록에 따라) --> 정답 지문

## 문장의 갯수 파악하기

core_feature_topic = ['should','could','have to', 'must', 'not', 'too', 'however', 'cannot', "can't", "don't", ]
key_words_topic = ['use', 'principle', 'information', 'elements', 'society', 'do', 'important', 'provide']
num_sent = len(text)

def correct_answer(text):
  sent_text = sentence_all(text)
  for sent in sent_text:
    sentence = ' '.join(sent)
    for feature in core_feature_topic:
      if bool(re.findall(feature, sentence)) is True:
        print('core feature를 가진 {}'.format(sentence))
        
    for keyword in key_words_topic:
      if bool(re.findall(keyword, sentence)) is True:
        print('key word를 가진 {}'.format(sentence))
      
      
      

# 4. 정답 문장 선별 이후, 유의어 사전을 활용한 paraphrasing

# 5. 오답 지문 생성은 랜덤하게 만들기?



## Pseudo code, 주제 지문 해설

# 1. 전체적인 맥락: 생성 키워드 고려 + 출현했던 동사와 명사 그리고 형용사 고려 

# 2. should, could, can / 부정어구, 부정문 / 빈도수가 높은 핵심 동사 명사 언급

# 3.

text = load_gen_text('Donghwan_vg')
sent_text = split_sentence(text)

sent_dataset = sentence_content(sent_text)

text

correct_answer(text)

text[:20000]

import random

random.choice(['중식', '미스터 국밥'])



