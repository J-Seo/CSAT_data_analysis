# -*- coding: utf-8 -*-
"""tf_idf.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Y2hlPmAbGZIyTf10k0mhlBldFD_RMvH4
"""

from operator import itemgetter
import numpy as np

import re
import openpyxl
import pandas as pd

# 1. 구글 드라이브에섯 기출 데이터셋 가져오기

from google.colab import drive
drive.mount('/content/gdrive')

# 2. 수능 기출 데이터 불러오기
# 3. 해당 경로의 데이터셋 열기 + 읽기 + utf-8로 해독하기



## 합쳐진 텍스트 파일 불러오기
merge_text = open('/content/gdrive/My Drive/Colab Notebooks/merge2.txt', 'rb').read().decode(encoding = 'utf-8')

## 개별 텍스트 파일 불러오기
file_names =['Donghwan_vg', 'Donghwan_bk', 'Jaehyung_sp.dms', 'Kanghee_at', 'Kanghee_ct', 'Minhyeong.dms' ]
file_path = []
def load_file_path(file_names):
  for file in file_names:
    text = '/content/gdrive/My Drive/Colab Notebooks/{}'.format(file)
    file_path.append(text)
  return file_path

file_path = load_file_path(file_names)

data_file = []

for name in file_path:
  data = open(name, 'rb').read().decode(encoding = 'utf-8')
  data_file.append(data)
  
## data_file[0] ~ data_file[5] 까지 실험데이터 저장
  
text = merge_text

# 4. 제대로 로드했는지 확인해보기
len(data_file[0]), len(data_file[1])

# 5. 정규표현식 적용

def erase_sc(text_data):
  text = re.sub('[-=+,#/\?:^$.@*\"※~&%ㆍ!』\\‘|\(\)\[\]\<\>`\'…》]', '', text_data)
  return text

def erase_hangul(text_data):
  text = re.sub('[ㄱ-ㅣ가-힣]+', '', text_data)
  return text

def erase_num(text_data):
  text = re.sub('[0-9]', '', text_data)
  return text

def only_eng(text_data):
  text = re.sub('[^a-zA-z\s]+', '', text_data)
  return text

text1 = only_eng(data_file[0])
text2 = only_eng(data_file[1])
text3 = only_eng(data_file[2])
text4 = only_eng(data_file[3])
text5 = only_eng(data_file[4])
text6 = only_eng(data_file[5])

# 6. 문서 내의 단어들의 출현 빈도
## row: 단어, column: 빈도수
def get_term_frq(document, word_dict = None):
    if word_dict is None:
        word_dict = {}
    words_ = document.split()

    for w in words_:
        word_dict[w] = 1 + (0 if word_dict.get(w) is None else word_dict[w])

    return pd.Series(word_dict).sort_values(ascending = False)

# 7. 문서별 각 단어가 몇 개의 문서에서 나타났는지 세기
## row: 단어, column: 문서1, 문서2
def get_docu_frq(documents):
    dicts = []
    vocab = set([])
    df = {}

    for d in documents:
        tf = get_term_frq(d)
        # 문서별 빈도수
        dicts += [tf]
        # dicts.append(tf)
        vocab = vocab | set(tf.keys())

    for v in list(vocab):
        df[v] = 0
        for dict_d in dicts:
            if dict_d.get(v) is not None:
                # 각 단어가 몇 개의 문서에 나타났는가
                df[v] += 1
    return pd.Series(df).sort_values(ascending = False)

# 8. 총 출현 횟수 / 문서별 출현 횟수: tf/idf --> idf에 해당하는 분모에서는 다른 문서에 많이 등장할수록 패널티 부여 
def get_tfidf(docs):
    vocab = {}
    tfs = []
    for d in docs:
        vocab = get_term_frq(d, vocab)
        tfs += [get_term_frq(d)]
    df = get_docu_frq(docs)

    stats = []
    for word, freq in vocab.items():
        tfidfs = []
        for idx in range(len(docs)):
            if tfs[idx].get(word) is not None:
                tfidfs += [tfs[idx][word] * np.log(len(docs) / df[word])]
            else:
                tfidfs += [0]

        stats.append((word, freq, *tfidfs, max(tfidfs)))

    return pd.DataFrame(stats, columns = ('word',
    'frequency', 'text1', 'text2', 'text3', 'text4', 'text5','text6', 'max')).sort_values('max', ascending = False)
  
tf_idf_file = get_tfidf([text1,text2,text3,text4,text5,text6])

tf_idf_file.to_excel('/content/gdrive/My Drive/Colab Notebooks/tf_idf.xlsx', sheet_name = 'sheet1')